{
  "agent_id": "coder3",
  "task_id": "task_3",
  "files": [
    {
      "name": "augmentation.py",
      "purpose": "Data augmentation techniques",
      "priority": "medium"
    },
    {
      "name": "feature_extraction.py",
      "purpose": "Feature extraction layers",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.HC_2508.20034v1_FlyMeThrough_Human_AI_Collaborative_3D_Indoor_Map",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.HC_2508.20034v1_FlyMeThrough-Human-AI-Collaborative-3D-Indoor-Map with content analysis. Detected project type: computer vision (confidence score: 9 matches).",
    "key_algorithms": [
      "Poi",
      "Current",
      "Mesh",
      "Feature",
      "Perceived",
      "Collaboration",
      "Steep",
      "Bined",
      "Camera",
      "Downsampled"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.HC_2508.20034v1_FlyMeThrough-Human-AI-Collaborative-3D-Indoor-Map.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nFlyMeThrough: Human-AI Collaborative 3D Indoor Mapping\nwith Commodity Drones\nXia Su\u2217\nUniversity of Washington\nSeattle, Washington, United States\nxiasu@cs.washington.eduRuiqi Chen\u2217\nUniversity of Washington\nSeattle, Washington, United States\nruiqich@uw.eduJingwei Ma\nUniversity of Washington\nSeattle, Washington, United States\njingweim@cs.washington.edu\nChu Li\nUniversity of Washington\nSeattle, Washington, United States\nchuchuli@cs.washington.eduJon E. Froehlich\nUniversity of Washington\nSeattle, Washington, United States\njonf@cs.washington.edu\nA  Fly\nB  User Annotate C  Locate\nD  ReviewReconstructed 3D Map \nRamp_1 \nInput Description \nFigure 1: We introduce FlyMeThrough , a drone-based indoor mapping system that semi-automatically maps indoor spaces\nand locates key facilities such as entrances, stairs, elevators, and doors. (A) We use a DJI Avata drone to scan indoor spaces,\nand generate reconstructed 3D maps with structure-from-motion (SfM) [ 40]; (B) We employ SAM2 [2] to enable intuitive\nand efficient user annotation of key indoor facilities. (C) FlyMeThrough auto-segments objects, generates annotations across\nsubsequent frames, and localizes them in the 3D map, which (D) users can interactively review and update.\nABSTRACT\nIndoor mapping data is crucial for routing, navigation, and building\nmanagement, yet such data are widely lacking due to the manual\nlabor and expense of data collection, especially for larger indoor\nspaces. Leveraging recent advancements in commodity drones and\nphotogrammetry, we introduce FlyMeThrough \u2014a drone-based in-\ndoor scanning system that efficiently produces 3D reconstructions\nof indoor spaces with human-AI collaborative annotations for key\nindoor points-of-interest (POI) such as entrances, restrooms, stairs,\n\u2217Both authors contributed equally to this research.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.and elevators. We evaluated FlyMeThrough in 12 indoor spaces\nwith varying sizes and functionality. To investigate use cases and\nsolicit feedback from target stakeholders, we also conducted a qual-\nitative user study with five building managers and five occupants.\nOur findings indicate that FlyMeThrough can efficiently and pre-\ncisely create indoor 3D maps for strategic space planning, resource\nmanagement, and navigation.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and\ntools ;Interaction design ;Visualization systems and tools .arXiv:2508.20034v1  [cs.HC]  27 Aug 2025\n\n--- Page 2 ---\nXia Su, Ruiqi Chen, Jingwei Ma, Chu Li, and Jon E. Froehlich\nKEYWORDS\nDrone; 3D reconstruction; Indoor Mapping; Video Segmentation;\nHuman-AI Collaboration\n1 INTRODUCTION\nIndoor mapping data for public indoor spaces ( e.g., office buildings,\ntrain stations, airports, and stadiums, etc.) is crucial for navigation,\nroute planning, space evaluation, and tracking spatial changes over\ntime. However, such data is often scarce and outdated [ 41]. Main-\nstream indoor mapping procedures involve two methods: transform-\ning CAD building plans into digital indoor mapping, or deploying\nspecific scanning hardware and services. However, these methods\nrequire high data acquisition costs and long timelines [ 70] and thus\nare hard to mass deploy and regularly maintain.\nIn contrast, drone-based indoor mapping, where drones fly\nthrough large indoor spaces to capture key spatial information,\noffers advantages in terms of cost and maintenance efforts. Demon-\nstrated feasible through both academic research [ 22,75] and com-\nmercial products like Skydio [55] and Elios [16], drone-based map-\nping can efficiently cover large and complex indoor spaces and\nhas the potential for higher levels of automation. However, ex-\nisting solutions often rely on LiDAR-equipped drones, which are\nprohibitively expensive for large-scale deployment ( e.g., anElios\n3LiDAR drone costs $50,000). Additionally, the generated indoor\nmaps lack points of interest (POI) that building management and\nvisitors care about. To address these challenges, we aim to develop a\nmore affordable and scalable drone-based indoor mapping pipeline\nthat only uses RGB video data, thus can potentially be captured\nby any commodity drone. In addition, we introduce a human-AI\ncollaborative annotation and location process that allows building\nmanagement staff to efficiently identify and mark key indoor POIs\n(e.g.entrances, stairs, elevators), making the output 3D maps more\nuseful for navigation, routing, and space evaluation.\nWe present FlyMeThrough , a human-AI collaborative indoor\nmapping system that provides an end-to-end pipeline for trans-\nforming RGB footage of indoor drone flights to POI-infused 3D\nreconstructed maps of the scanned indoor spaces. FlyMeThrough is\ncomprised of three major technical components: first, a SfM (Struc-\nture from Motion)-based 3D reconstruction [ 40] that transforms\ninput RGB videos to estimated camera positions of video frames as\nwell as 3D mesh models of the indoor spaces. Second, a human-AI\ncollaborative annotation pipeline that enables users to efficiently\nannotate and locate key indoor POIs. Third, a web interface that\nreviews the final results, which are 3D models with bounding boxes\nindicating the locations and dimensions of key indoor POIs. All\ncomponents of the system are released and open-sourced.1\nTo evaluate FlyMeThrough, we collected drone footage from 12\nindoor spaces of varying sizes and functionalities under the ap-\nproval and guidance of building managers, who are responsible\nfor maintaining these scanned indoor spaces. We also conducted a\nuser study with these five building managers along with five active\nbuilding occupants of these scanned spaces, to assess the perfor-\nmance and usefulness of the annotation interface. We presented\nour drone-based indoor mapping process, invited participants to\nannotate the collected data for key indoor POIs, then interviewed\n1https://github.com/makeabilitylab/FlyMeThroughthem to assess the usability and performance of our custom pipeline.\nThe study results show high technical performance and usability\nof our mapping system, while revealing application scenarios and\naspects of future improvements.\nIn sum, our contributions are threefold: first, we designed and\nimplemented the first drone-based indoor mapping system that\nleverages only RGB data\u2014enabling the use of affordable, off-the-\nshelf consumer drones for large-scale indoor reconstruction. Sec-\nond, we introduce a custom human-AI collaborative annotation\npipeline that allows users to flexibly define and create POIs in re-\nconstructed indoor 3D maps. Finally, our user study with building\nmanagers and occupants contributes key insights into practical\nneeds, usability, and potential applications of drone-based indoor\n3D mapping systems for future work.\n2 RELATED WORK\nWe situate our work in research on large-scale indoor mapping,\ndrone-based 3D reconstructions, and techniques to automatically\ndetect and localize objects in scenes.\n2.1 Large Space Indoor Mapping\nLarge indoor spaces, such as train stations, airports, malls, and office\nbuildings, require high-quality spatial maps to support applications\nlike navigation, space management, and digital twin construction.\nMainstream commercial indoor mapping services like Pointr [42],\nESRI [13],Mappedin [33], and Mapsted [34] typically rely on exist-\ning or user-drawn architectural data as input to generate interactive\nindoor navigation and management systems. Meanwhile, 3D in-\ndoor scanning services like Matterport [1],Cupix [11], and NavVis\n[38] utilize LiDAR-equipped professional devices to provide high-\nprecision 3D reconstruction and virtual visualization. In recent\nyears, applications like PolyCam [43] and LumaAI [29] have ex-\nplored the creation of digital twins for small to medium scale indoor\nspaces using mobile LiDAR or RGB videos, while APIs such as Ap-\nple\u2019s RoomPlan [5] further lower the threshold for small-scale 3D\nscanning. Although these commercial solutions are technologically\nmature, they still exhibit significant limitations. On the one hand,\ntheir reliance on professional hardware or manual data collection\nleads to high costs for data acquisition and subsequent updates. On\nthe other hand, mobile-based solutions are typically designed for\nsmall indoor spaces and cannot efficiently scale to large environ-\nments spanning thousands of square meters.\nIn contrast to commercial approaches, recent academic research\nhas proposed a variety of open-sourced 3D mapping methods, ex-\nploring how to leverage lower-cost devices for indoor space recon-\nstruction. These include methods based on 360-degree cameras [ 21],\nGNSS-assisted spatial localization and mapping [ 71], smartphone\nRGB video and LiDAR [ 60], and robot-based autonomous scanning\nsystems [ 56,58]. In terms of reconstruction algorithms, traditional\nSfM and Multi-View Stereo methods ( e.g.,COLMAP [49,52]) remain\nwidely used, while recent advances in 3D Gaussian Splatting [20,24]\noffer new possibilities for high-quality RGB-only 3D reconstruction.\nNevertheless, most existing approaches still rely heavily on man-\nual operation or ground-based equipment, which limits their au-\ntomation capability, mapping efficiency, and scalability in large and\ncomplex indoor environments. In this work, we aim to propose an\n\n--- Page 3 ---\nFlyMeThrough\nindoor mapping method that can efficiently scale to large indoor\nspaces with affordable hardware.\n2.2 Drone-based Indoor Mapping\nPast research [ 3,22,23,32,48,73,75] has explored the feasibility of\nusing drones to map indoor spaces. A common research methodol-\nogy in this thread is equipping drones with rich sensing capabilities\nor attaching additional sensors, such as RGBD cameras ( e.g., In-\ntel RealSense D435i ) or even LiDAR scanners ( e.g., DJI Zenmuse\nL2), to enhance spatial perception and reconstruction accuracy. In\naddition to academic research, commercial solutions like Skydio\n[55] and Flyability Elios [16] adopt a similar hardware strategy,\nleveraging LiDAR and IMU sensors mounted on drones to enable\naccurate indoor 3D reconstruction. While these methods and prod-\nucts achieve high-quality mapping results, they usually rely on\nexpensive hardware setups and involve substantial technical effort\nin data acquisition and processing. Such requirements limit their\nscalability and general applicability, especially in scenarios where\nlow-cost, lightweight, and easy-to-deploy solutions are desired.\nIn contrast to existing drone-based indoor mapping approaches,\nour work targets broader applicability by minimizing hardware con-\nstraints. Specifically, we propose a drone-based 3D indoor mapping\npipeline that relies solely on RGB data, enabling most off-the-shelf\nconsumer drones to be utilized for large-scale indoor reconstruction.\nThis choice is deliberate: by demonstrating robust performance with\nRGB-only input, our method offers a low-cost, hardware-agnostic\nsolution that remains practical in low-resource scenarios where\nhigh-precision stereo or depth sensors may not be available. This\ndesign lowers the barrier to adoption and extends the applicability\nof our approach across research, education, and industry contexts.\n2.3 Detecting and Locating Real-world Objects\nAccurately detecting and localizing key objects and facilities is\nessential for creating semantically rich indoor 3D maps [ 25,26,\n54,65,74]. Traditional approaches have relied on manual annota-\ntion through GIS or digital twin tools, but these methods typically\ndemand professional expertise and present steep learning curves\nfor users [ 4,45,76]. While effective in certain cases, these manual\nmethods are time-consuming, expensive, and difficult to scale.\nRecent advances in computer vision have enabled more auto-\nmated approaches for object detection and localization [ 25,26,53,\n59,74]. For example, Project Sidewalk [28,47,67] demonstrates this\nby crowdsourcing sidewalk feature annotations to train object de-\ntection models that automatically extract data from street views.\nSimilarly, in indoor environments, RASSAR [60] employs object\ndetection with raycasting to identify and locate smaller indoor ob-\njects relevant to accessibility and safety. Among such automated\napproaches, YOLO (You Only Look Once) [64] and its variants have\nbecome widely adopted for their efficiency and accuracy in detect-\ning predefined object categories.\nHowever, existing automated approaches [ 25,53,68] face critical\nlimitations when applied to large and diverse indoor environments,\nwhere important objects are visually distinctive and universally\ndefined \u2014 assumptions that often do not hold in practice. YOLO,\nfor example, is a closed-set detector that can only recognize objectcategories included in its training vocabulary, limiting its appli-\ncability to customized and context-specific indoor environments.\nIn practice, indoor spaces often contain diverse, customized, and\ncontext-specific objects, mainly depends on the situated knowledge\nof local users that purely AI-driven approaches cannot easily infer\nor replicate.\nTo address these challenges, we leverage SAM2 [44] to oper-\nationalize a paradigm shift in semantic indoor mapping \u2014 from\ntreating it solely as an automated perception task to framing it as\na collaborative knowledge construction process between humans\nand AI. Inspired by recent works on the human-robot collaboration\napproach for 3D semantic labeling [ 10,46,72], we emphasize the\nvalue of integrating human expertise with AI capabilities to create\nricher and more accurate indoor semantic maps.\nTo operationalize this vision, we introduce a human-AI collabora-\ntive annotation workflow that engages local occupants and building\nmanagers in the map creation process. Rather than relying on fixed-\ncategory, pre-trained object detectors, our approach empowers\nusers to flexibly define and annotate objects based on their situated\nknowledge and contextual needs. Ultimately, this collaborative ap-\nproach repositions indoor semantic mapping as a socio-technical\nprocess \u2014 balancing the efficiency of AI automation with the inter-\npretive agency of human users \u2014 to ensure that the resulting maps\nare both semantically meaningful and grounded in the complexities\nof real-world indoor environments.\n3 THE 3D MAPPING OF INDOOR SPACES\nFlyMeThrough is a multi-stage system that transforms RGB drone\nfootage into interactive 3D indoor maps annotated with key Points\nof Interest (POIs) (Figure 1). The system begins with drone-based\nRGB video collection, followed by Structure-from-Motion (SfM) to\nestimate camera trajectories and reconstruct the space into a 3D\nmesh. Users such as building managers then annotate key indoor\nfacilities ( e.g., stairs, doors, elevators) on selected video frames,\ninformed by their grounded knowledge of the space. These annota-\ntions are expanded into per-frame video segmentation masks using\nSAM2 [ 44], and these masks are subsequently projected into the\n3D space using a depth-guided raycasting algorithm. The result is\na semantically enriched 3D map with interactively viewable POI\nbounding boxes that indicate both the location and dimensions of\neach annotated facility.\nThis work extends our previous poster paper [ 59], which ex-\nplored the feasibility of using commodity drones for indoor 3D\nmapping and automatic facility detection of stairs, doors, elevators,\nand ramps. While retaining core elements such as RGB video cap-\nture and SfM-based reconstruction, FlyMeThrough introduces a\nmore robust human-AI collaborative architecture. Our new anno-\ntation pipeline and depth-based localization significantly improve\nthe semantic fidelity of the 3D maps, enabling end-users to mean-\ningfully embed spatial knowledge into the resulting reconstruction.\n3.1 Drone-based data collection\nIn line with prior work, we opt for RGB-only video input for its\npracticality and broad compatibility with consumer drone hardware.\nFor safety and controllability during indoor flights, we primarily\nused a DJI Avata drone (cost $759 when purchased in 2024) as the\n\n--- Page 4 ---\nXia Su, Ruiqi Chen, Jingwei Ma, Chu Li, and Jon E. Froehlich\nA   Drone B   Footage \nC   Extract Frames 2FPS D   Estimate Camera Extrinsics \nE   3D Reconstruction  \nFigure 2: Drone-based indoor scanning and reconstruction.\n(A) The DJI Avata drone. (B) We collect footage of flights as\n4K 30FPS videos. (C) We then extract frames from the footage\nat 2FPS. (D) We use Agisoft Metashape to estimate camera\nextrainsics of the extracted frames and also conduct (E) 3D\nreconstruction of the space.\nmain testing drone for its propeller guards and immersive con-\ntroller headset. We select DJI Avata\u2019s wide-angle camera setting\nwhich yields a 110-degree field of view, and capture videos during\nindoor flights at 4k resolution and 30 frames per second. To evaluate\nthe minimum requirements for effective mapping, we also tested\nour pipeline with a less advanced consumer drone, the DJI Mini\n2(released in 2020, $450), which records 2.7K, 30FPS videos with\na smaller 83-degree FOV and a 12MP camera. Despite its lower\nspecifications, the pipeline still performed effectively. Based on\nthese tests, we recommend drones with at least a 12MP camera and\n30FPS video capability for robust results. The lead author manually\ncontrols the drone to steadily fly through the public open spaces in\nour tested buildings.\n3.2 SfM Indoor Reconstruction\nFollowing the same settings as our previous work [ 59], we trans-\nform the drone footage into image frames at 2 frames per second\nto conduct indoor reconstruction. We systematically evaluated a\nrange of reconstruction methods, including state-of-the-art open-\nsource solutions such as NeRF [36,61],Gaussian Splatting [24],\nCOLMAP-based SfM [50,51], as well as learning-based approaches\nlikeMV-DUSt3R+ [62] and MAST3R-SLAM [37].\nOur experiments revealed that COLMAP struggles to reconstruct\nbuilding-scale indoor scenes due to cumulative drift and the lack\nof loop closure [ 50], which leads to error propagation in large and\nrepetitive environments. NeRF [ 36] and Gaussian Splatting [ 24],\nwhich rely on COLMAP for camera pose estimation, inherit these\nlimitations and exhibit degraded performance in large-scale settings.\nSLAM systems such as DROID-SLAM [63] reconstruct large-scale\nscenes online and refine them through global optimization, but their\naccuracy remains constrained by the quality of initial estimates.\nRecent feedforward approaches like MAST3R-SLAM [ 37], which\nleverage learned priors for reconstruction, operate on two-frame\ninputs and often produce inconsistent geometry that is difficult to\ncorrect in post-hoc optimization.\nUser Annotate \nFrame 77 \nFrame 78 Frame 79 Frame 80 Figure 3: We employ SAM2 to segment key indoor facilities.\nWhen user annotate in one frame, the mask will be propa-\ngated to subsequent frames.\nBased on these evaluations, we selected the proprietary pho-\ntogrammetry software Agisoft MetaShape [39] as the most robust\noption in terms of output quality, reconstruction success rate, and\ncomputational efficiency. MetaShape performs SfM camera pose\nestimation and 3D reconstruction reliably for large and complex\nindoor spaces. Figure 2E shows the reconstructed 3D model, and\nFigure 2D shows the estimated camera pose and flight trajectory.\n3.3 Indoor Facility Segmentation\nTo embed key facility information into the reconstructed 3D map,\nFlyMeThrough employs the SAM2 [ 44] video segmentation model\nto interactively identify and label key indoor objects. For each scene,\nusers make intuitive clicks on the video frames to annotate key\nindoor objects, including pre-selected types drawn from existing\nindoor scanning work [ 60]:door,elevator ,ramp ,stairs ,etc., as well\nas any customized categories named by users.\nThis design marks a departure from our previous system [ 59],\nwhich relied on YOLOv8 [ 64] for fully automated POI detection.\nWhile effective in automating object recognition, YOLOv8 lacked\nflexibility for open-vocabulary object types and precision for multi-\nframe consistency, especially in novel or complex indoor scenes.\nIn contrast, SAM2 offers several key advantages that informed our\ndecision to adopt it:\nInteractive and Efficient Annotation : SAM2 natively sup-\nports interactive prompts\u2014such as point clicks and bounding boxes\n\u2014 which integrate seamlessly into our UI and provide intuitive\nuser interaction while minimizing annotation effort. With mini-\nmal user input, the model generates high-quality pixel-level masks,\nsignificantly reducing manual efforts.\nGeneralization and Adaptability : While YOLOv8 is limited\nto closed-set object categories, SAM2 shows strong generalization\nacross diverse object types and indoor layouts. Its zero-shot seg-\nmentation capability enables accurate labeling of novel object in-\nstances\u2014ideal for dynamic and unstructured indoor settings as well\nas varied user requirements.\nPrecision and Continuity : SAM2 produces pixel-accurate seg-\nmentations that capture fine object boundaries. This granularity is\n\n--- Page 5 ---\nFlyMeThrough\ncritical for downstream ray-casting-based localization, where geo-\nmetric precision determines placement accuracy in the 3D space.\nAlso, the results are video segmentations that propagate across adja-\ncent frames, which improve temporal consistency for the following\nlocalization step.\nIn deployment, we employ a pretrained SAM2 checkpoint [ 2],\nwhich tokenizes all input video frames and infers object masks in\nfollowing frames based on a user-provided segmentation (See Fig-\nure 3 for an example). To enhance temporal consistency, we adopt\na local temporal control strategy. Specifically, segmentation predic-\ntions of a user annotation are retained only for consecutive frames\nand are terminated when the target object exits the view. This helps\nmitigate missegmentations caused by repetitive layouts\u2014a common\nchallenge in hallways or mirrored environments.\n3.4 Localizing Indoor Features with\nDepth-guided Ray-casting\nThe segmentation results, as masks over sequential video frames,\nare projected into the reconstructed 3D maps for localization. In our\nprevious system, this projection was performed by individual pixels\nfollowing the classical pinhole camera model [ 57]. Each masked\npixel was back-projected through the camera\u2019s intrinsic and extrin-\nsic parameters to cast a ray into the 3D scene. Intersections between\nthese rays and the reconstructed surface mesh were computed, and\nthe resulting hit points were post-processed via spatial clustering\nto generate 3D bounding boxes for each object instance. However,\nthis approach was highly sensitive to mesh artifacts, such as surface\nholes and noise, which frequently led to fragmented or inaccurate\nlocalization results.\nTo address this, we introduce a depth-guided ray-casting strat-\negythat leverages monocular depth estimation to constrain ray\nlengths within each image mask. This process generates a segmen-\ntation point cloud to be raycasted as a whole, thereby significantly\nimproving localization accuracy and suppressing false intersections\ncaused by mesh artifacts.\nClassical Pinhole Projection Model We retain the pinhole\ncamera model, where a 3D point \u00ae\ud835\udc43\ud835\udc64=(\ud835\udc4b\ud835\udc64,\ud835\udc4c\ud835\udc64,\ud835\udc4d\ud835\udc64)\ud835\udc47is projected\nto a image coordinate (\ud835\udc62,\ud835\udc63)with camera intrinsic matrix \ud835\udc3eand\nextrinsic parameters \ud835\udc45and\ud835\udc61, following:\n\ud835\udc4d\ud835\udc50\u00b7\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ud835\udc62\n\ud835\udc63\n1\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb=\ud835\udc3e\u00b7[\ud835\udc45|\ud835\udc61]\u00b7\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ud835\udc4b\ud835\udc64\n\ud835\udc4c\ud835\udc64\n\ud835\udc4d\ud835\udc64\n1\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb(1)\nGiven known \ud835\udc3e,\ud835\udc45, and\ud835\udc61from the SfM reconstruction, the only\nmissing component is the depth \ud835\udc4d\ud835\udc50value, which is traditionally\ncalculated by raycasting.\nCreate Segmentation Point Cloud with Depth Estimation\nAlthough depth is not directly available in monocular RGB videos,\nwe can still estimate a relative depth for each pixel point in the\nsame image frame using a state-of-the-art monocular depth predic-\ntion model Depth Pro [ 6]. We then use the predicted depth values,\nindicated as \u02c6\ud835\udc4d\ud835\udc50, to extrude the pixels in the segmentation mask to\n3D coordinates\u02dc\u00ae\ud835\udc43\ud835\udc64:\nDepth Point \nCloud\nScale = 1Scale=\n(1.01)^9Scale=\n(1.01)^13Scale=\n(1.01)^16Scale=\n(1.01)^21\nA  B   \nC  Anonymized\nFigure 4: We employ a depth-guided ray-casting to locate user\nannotations. (A) Each annotation mask will be incorporated\nwith (B) inferred depth map to produce a 3D point cloud, (C)\nwhich gets iteratively enlarged until intersection with the\nreconstructed 3D model.\n\u02dc\u00ae\ud835\udc43\ud835\udc64=\ud835\udc45\u22121\u00b7\u00a9\u00ad\n\u00ab\u02c6\ud835\udc4d\ud835\udc50\u00b7\ud835\udc3e\u22121\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ud835\udc62\n\ud835\udc63\n1\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\u2212\ud835\udc61\u00aa\u00ae\n\u00ac(2)\nThis process produces a 3D point cloud from pixels in a 2D\nsegmentation mask, which captures the spatial extent and relative\ndepth landscape of the object in the scene. See Figure 4 for an\nexample. Note that the resulting point cloud is not metrically scaled\nwith the mesh model, as it is derived from predicted monocular\ndepth with an unknown global scale factor, which we iteratively\napproximate in the following casting steps.\nUnified Segmentation Point Cloud Casting Rather than ray-\ncasting each individual pixel (as in our prior approach) which can\nbe stopped by irregular geometry or go through holes, we cast the\nsegmentation point cloud as a whole. See Figure 4. We iteratively\nexponentiate the scale of the point cloud by 1.01 and calculate the\ncorrelation portion with the reconstructed mesh, until it reaches\na threshold (currently set as a fixed 22% based on experiments).\nThis casting strategy avoids sensitivity to local mesh defects (holes,\nclutter, irregular geometry), and helps preserve the object\u2019s shape\nand orientation in the casting results. This prevents premature or\ndelayed intersection errors, and mitigates offset artifacts introduced\nby surface irregularities. Compared with our previous system, the\ncasting process achieves higher geometric accuracy and better se-\nmantic consistency within complex indoor 3D models.\nAdaptive Downsampling for Efficiency To optimize runtime\nperformance during the cloud-based raycasting steps, we intro-\nduce adaptive spatial downsampling. We reduce the point cloud\nsize while preserving its overall shape: The number of retained\npoints is dynamically selected. Larger objects retain more points\n\n--- Page 6 ---\nXia Su, Ruiqi Chen, Jingwei Ma, Chu Li, and Jon E. Froehlich\nto ensure precision, while smaller ones are compactly represented.\nThis balance significantly reduces computation while maintaining\nsufficient spatial fidelity.\n3.5 Bounding Box creation\nAfter raycasting the downsampled segmentation cloud into the\nmesh, the resulting intersection points are clustered to generate a\nfinal 3D bounding box. This bounding box encapsulates not only\nthe object\u2019s location, but also its approximate size and orientation\nin 3D space.\nSince we raycast video segmentation results across multiple\nframes for the same object to further even out noise, this multi-\nframe casting result is a densely overlapping point cloud data. We\napply DBSCAN [14] to segment and filter the point cloud, removing\noutliers and preserving the most coherent geometric regions associ-\nated with the object. We then compute a PCA-based approximation\nof the Minimum Volume Bounding Box [ 12] for each cluster, which\nproduces tighter, direction-aware bounding volumes that better\nreflect the true geometric extent of the object, especially under\nnon-uniform orientations.\n3.6 Modularity of Implementation\nAs a multi-step pipeline that includes commodity hardware, com-\nmercial software, classical algorithms, state-of-the-art models, as\nwell as our original algorithms and methods, we realize the po-\ntential limitation and space for future improvements in all these\ncomponents. In this case, we have designed our system to be modu-\nlarized so that each step or component can be replaced with better-\nperforming versions in the future. For example, while our current\nimplementation uses monocular RGB inputs with estimated depth,\nthe depth-estimation module can be seamlessly swapped with real\ndepth data if the drone is equipped with onboard depth sensors.\nSimilarly, the 3D reconstruction module\u2014currently based on com-\nmercial photogrammetry software\u2014can also be replaced with open-\nsource or more advanced reconstruction methods as they become\navailable. This modularized architecture enables customized imple-\nmentation by future users to better adapt to their specific hardware,\nsoftware, and also compute power limitations.\n4 THE FLYMETHROUGH INTERFACE\nAs elaborated in the previous section, FlyMeThrough engages hu-\nman users to provide ground knowledge, i.e.,identifying what and\nwhere the most important and relevant indoor facilities are in the\nreconstructed indoor 3D maps. This crucial mapping task cannot be\nfully automated with computer vision due to two reasons. For one,\nfacilities in indoor spaces vary in appearance, making them hard\nto be accurately detected by generalized models; For another, the\nfunctions and importance of indoor facilities are highly specific and\npersonalized in the site-specific practices. In this case, we designed\nand implemented an annotation interface to enable human-AI col-\nlaborative creation of POIs in reconstructed 3D indoor maps. We\nalso created a review interface that enables users to review the\noutput maps as interactive 3D models.4.1 Annotation interface\nWe create a web interface that enables efficient and intuitive in-\ndoor POI annotation. Unlike traditional indoor mapping process\nthat leverages CAD files and requires relevant design skills, our\nweb interface shows the image frames of the drone footages and\nenables click-based annotation operations to enable user to mark\nany target objects, like entrances, doors, and stairs, in the image\nframes. Figure 5 shows the annotation interface, which include four\nmain components: Annotation canvas (Figure 5B) that shows image\nframes, annotation points and masks, as well as bounding boxes of\nprocessing results of annotations. Frames panel (Figure 5D), which\nlists image frames of the drone footage in the order of time. An-\nnotation tools panel (Figure 5C), which includes types of objects\nfor users to select from, and action buttons for specific annota-\ntions and the overall annotation task. Annotation results panel\n(Figure 5A), which lists the user-confirmed annotations, their pro-\ncessing progress, and also enables reviewing of annotations.\nWhen annotating, users browse through the frames panel and\nselect video frames to annotate. They can create object instances\nwith the annotation tools panel from either pre-defined object types,\nor by creating new object types. For each object instance, users\ncan optionally input a more detailed description and click on the\nannotation canvas to select the target object. A SAM2 [ 44] model\nembedded in the web interface will provide a real-time segmenta-\ntion mask, which can be revised by adding more positive points\nor negative points. Positive points are added by clicking on the\nnon-masked parts of the canvas, and negative points by clicking\non the masked parts. Users can also clear points to restart, remove\nthe instance entirely, or, when they are satisfied with the anno-\ntation, click confirm to send this annotation to our server, where\nthe mask will be propagated to subsequent video frames by video\nsegmentation [ 44]. The segmentation results, when finished, will be\nvisualized as bounding boxes in the subsequent frames (Figure 5E),\nand also further processed by raycasting (see subsection 3.4) to\ncreate 3D bounding boxes in the reconstructed 3D mesh.\n4.2 Visualization interface\nWe also implement another web interface to visualize the 3D indoor\nmaps, which include 3D mesh models that reconstruct the scanned\nindoor space, and also 3D bounding boxes that mark the location\nand dimension of annotated indoor objects. Figure 6 shows this\ninterface, containing a space list panel (Figure 6A) for available\nindoor maps, a 3D interactive canvas (Figure 6B) that shows the\nindoor map, and also a right panel (Figure 6C) that lists all annotated\nfacilities, as well as a floor plan view of the 3D map rendered as a\ntop-down orthogonal view of the 3D map. Users can click and drag\nto rotate view angles, browse through a list of objects, hover on\nthem to see them highlighted in the 3D canvas, and also click any\nobject to focus the view on it.\n5 EVALUATION\nWe evaluate FlyMeThrough for its technical performance and over-\nall usability. We collected drone footage of 12 spaces with varying\nspace types, sizes, vertical heights, and also functionalities. We pro-\ncess these spaces into 3D indoor maps to understand the technical\ncapability of our pipeline. We also evaluate interface usability with a\n\n--- Page 7 ---\nFlyMeThrough\nA  B   Canvas C   Annotation Panel \nD  Frames E   \nF   \nFigure 5: The annotation interface. (A) Annotation results panel shows all confirmed annotations. (B) Canvas shows the selected\nframes and enables click-based annotation for segmentation masks, which will be highlighted with color. (C) Annotation panel\nshows object types to choose from and annotation actions like Clear Points, Comfirm Annotation , as well as final actions like\nFinish Annotation . (D) Frames panel lists all video frames available for annotation. (E) After user confirm an annotation, it gets\nprocessed by our segmentation server in real time and the returned segmentation results for subsequent frames will be shown\nas bounding boxes. (F) After finishing the annotation, interface shows a summary.\nA   Space List B   3D Viewport C  Facility List \nTop View Building Name \nAnonymized \nBuilding Name \nAnonymized \nBuilding Name \nAnonymized \nBuilding Name \nAnonymized \nFigure 6: The review interface. (A) A list of spaces available\nto review. (B) A 3D viewport shows the 3D map and anno-\ntated objects. (C) List of annotated facilities, each can be\nhighlighted. Click on the \"View\" button will focus the view\nto the bounding box. Interface also provide a button to go\nback to bird view, and an orthographic top view of the map\nas a floor plan of the space.\nstudy among 10 participants, including five building managers and\nfive building occupants, which also aims to understand the practical\nimplications and potential barriers to implementing drone-based\nindoor mapping systems.\n5.1 Procedure\nIn order to conduct indoor drone flights, we first reached out to\nbuilding managers to authorize and supervise us to fly our drone intheir buildings. With approval, the lead author manually controlled\nthe drone to capture building spaces at off-hours to minimize risk\nand disruption. We collected flight footage of 12 indoor spaces with\nvarying sizes, space types, and building functionalities. See Table 1\nfor more details. We then reconstructed the spaces with our system\nto test robustness.\nTo evaluate the usability and performance of our interface, we\nconduct a user study which guides participants to conduct POI an-\nnotation and 3D map review. The study is conducted through both\nremote and in-person interviews, consisting of five sequential parts:\n(1) initial set up, (2) drone mapping demonstration, (3) annotation\ninterface experience, (4) initial interview and discussion, and (5)\nfinal model review and feedback. Each session lasted ~60 minutes.\nThe first part begins with meeting the participant, reviewing and\nobtaining informed consent, and conducting a brief background\nsurvey. Building managers are asked additional questions about\ntheir current building evaluation practices. In the second part, if the\nparticipants have not witnessed the drone flying process, they are\nshown a comprehensive demonstration of the drone-based mapping\nprocess through recorded footage from both the drone\u2019s perspective\nand a bystander\u2019s view. This dual-view presentation help partici-\npants understand both the drone\u2019s operational capabilities and its\npresence in the space. The third part focused on the annotation\ninterface, where participants were given hands-on experience with\nthe annotation process. In the fourth part, we ask a set of open-\nended questions to assess participants\u2019 reactions to the drone flight\nprocess, evaluate the annotation system\u2019s usability, and explore\nhow this technology could be integrated into their existing building\nevaluation practices or provide value in other applications. During\nthis discussion period, the system processes the collected data in\n\n--- Page 8 ---\nXia Su, Ruiqi Chen, Jingwei Ma, Chu Li, and Jon E. Froehlich\nS1   \nS2    S4    \nS5    S6    S8    S10    \nS12    \nS11    \nFigure 7: Nine of the scanned spaces.\nTable 1: Overview of Spaces with Video Data and Reconstruction Status\nSpace ID Function Area (sqm) Height (floors) Video Length Reconstruction Status\nS1 Education 1600 6 3min45s Good\nS2 Education 1400 4 2min33s Good\nS3 Education 100 1 2min08s Good\nS4 Office 120 1 3min22s Good\nS5 Education 180 1 2min10s Good\nS6 Education 600 1 3min06s Good\nS7 Exhibition 3000 2 6min07s Good\nS8 Engineering 260 3 5min03s Good\nS9 Office 1100 4 5min33s Failed\nS10 Engineering 2000 1 5min53s Good\nS11 Education 800 1 4min02s Good\nS12 Education 2400 3 9min13s Good\nthe second part into an instance-embedded 3D model. The final\npart began once the 3D model is ready. We demonstrate the final\nresult through our interactive interface, showing participants how\nthe drone-captured data translates into a usable building model.\nThe session concluds with additional questions about their overall\nexperience with the system and their assessment of its performance\nin relation to their respective needs.5.2 Participants\nWe recruited five building managers to support our drone flights. All\nof these building managers are also interviewed in our user study.\nAdditionally, we recruited five building occupants who are active\nusers of the scanned buildings. Demographics. Among these partic-\nipants, four witnessed our drone operations, and the rest watched\nvideo recordings. Participants are compensated $25 per hour for\nguiding and witnessing the drone flights and for participating in\nour user study. See Table 2 for more demographics information.\n\n--- Page 9 ---\nFlyMeThrough\nTable 2: Participant Demographics and Annotation Performance\nParticipant Age Gender Role Space Tested Experience (yrs) Footage/Witness Annotations Success (%)\nM1 35-44 M Building Manager S1 13 Footage 12 100.00\nM2 45-54 M Building Manager S8, S10 7 Witness 33 75.76\nM3 25-34 M Building Manager S2 9 Footage 19 100.00\nM4 18-24 F Building Manager S11 1.5 Witness 16 87.50\nM5 55+ M Building Manager S12 7 Footage 9 100.00\nO1 25-34 M Occupant S6 5 Witness 29 93.10\nO2 25-34 M Occupant S2 5 Footage 8 87.50\nO3 25-34 M Occupant S4 3.5 Witness 8 100.00\nO4 25-34 M Occupant S5 0.5 Footage 14 100.00\nO5 25-34 F Occupant S1 6 Footage 10 100.00\n5.3 Analysis Approach\nOur analysis of the semi-structured interviews focused on sum-\nmarizing high-level themes. One researcher developed a set of\nthemes through qualitative open coding [ 8] based on the video\ntranscript, then coded the responses according to the themes. Par-\nticipant quotes have been slightly modified for concision, grammar,\nand anonymity.\nAll annotations, segmentations, as well as subsequent raycasting\nresults, are logged and recorded during user studies. The research\nteam analyzes such data for computation efficiency and robustness.\n5.4 Technical Results\nAmong the 12 target spaces, reconstruction succeeded in 11 cases,\nwith only 1 failure. The single failure (S9) likely resulted from\nits challenging architectural characteristics: a circular, multi-floor\ncorridor with highly repetitive vertical and horizontal structures,\nwhich may have hindered reliable feature detection and matching\nduring the SfM process (see Table 1 for space details and Figure 8A\nfor pictures). Figure 7 shows nine of the 11 reconstruction results.\nDuring the user study sessions, participants created an average\nof 15.8 annotations per space (individual counts are provided in\nTable 2). All annotations were successfully processed by our SAM2\nmodel, with an average processing time of 41.10 seconds per an-\nnotation (SD = 11.21). For the ray-casting module, each casting\noperation took an average of 63.6 seconds (SD = 48.65). Notably,\n91.77% of all annotations were successfully cast into bounding boxes\nfor users to review. Annotation counts and casting success rates\nper participant are summarized in Table 2.\nWe also analyzed the failed casting cases. A total of 13 failures\nwere recorded, of which 8 (61.5%) were due to missing camera\nparameters in the corresponding frames at the beginning of the\nSfM-based indoor reconstruction process. Another five additional\nfailures (38.5%) were caused by SfM reconstruction being partially\nincomplete in some cases, leading to missing geometry. See Figure 8\nB and C for examples.\n5.5 Findings\nA total of 158 objects were annotated by the participants. Among\nthem, doors (including regular and rolling doors) were the most fre-\nquently labeled (31), followed by common building infrastructure\nsuch as stairs (21), entrances (10), elevators (9), restrooms (5), and\nA1    B   \nC   A2    Figure 8: Three types of failure cases. (A) A1 and A2 show two\nimages from S9, which is an office building with a multi-floor\natrium space. The reconstruction of this space failed. (B) The\nreconstructed 3D map can be blurry on less scanned parts. (C)\nLeft shows the reconstruction missing certain parts, e.g.the\ntrash collector shown in the original footage image shown\non the right.\nramps (3). In addition to these core architectural features, partici-\npants also annotated a wide range of smaller objects ( e.g., signage,\nfurniture, equipment). We also observed a clear distinction in an-\nnotation focus between building managers and building tenants:\nmanagers tended to label more technical and safety-related fea-\ntures ( e.g., fire control systems, equipment panels), whereas users\nmore frequently annotated everyday-use elements such as sofas,\nwhiteboards, and trash bins.\nBelow we summarize the major themes that emerged from our\nuser interviews.\n5.5.1 Annotation System Usability and Areas of Improvement. Our\nevaluation of FlyMeThrough\u2019s annotation interface revealed gener-\nally positive feedback regarding its usability. When asked to rate\nease of use on a 7-point Likert scale (1=very difficult, 7=very easy),\nparticipants reported a median rating of 6 ( IQR=2.00), indicating\nan overall favorable assessment of the system\u2019s usability. Several\nparticipants explicitly characterized the interface as intuitive, with\nbuilding managers and occupants noting it was \u201cpretty easy to use\u201d\n\n--- Page 10 ---\nXia Su, Ruiqi Chen, Jingwei Ma, Chu Li, and Jon E. Froehlich\n(M2, O5) and \u201cnot complicated at all\u201d (M5). This suggests that Fly-\nMeThrough\u2019s annotation workflow can accommodate users with\nvarying levels of technical expertise.\nDespite overall positive reception, we also identified areas of\nimprovement. A common challenge involved object selection work-\nflow: \u201cThe confusing thing is you have to click this first and then\nidentify the thing again... the order is maybe a little bit confusing\u201d\n(M1). Similarly, M4 noted: \u201c Sometimes I would forget to [select the\nobject type first]. I would click on an object and then realize I didn\u2019t\nselect what type of object it was.\u201d Participants also expressed interest\nin additional functionality that could enhance their experience. For\nexample, O2 suggested implementing an example-based approach:\n\u201cI think for the annotation part, it might be helpful if I can be provided\nwith examples so I don\u2019t have to start on my own. I can just say yes or\nno.\u201d This comment points to the potential benefit of incorporating\nautomated annotation features with human-verification.\n5.5.2 Perceived Model Quality. When asked to rate the quality of\nthe model on a scale of 1-7 (1=poor, 7=excellent), participants had a\nmedian rating of 5 ( IQR=0.75), indicating generally positive percep-\ntions of the system\u2019s output quality. We also asked participants to\ngive a binary rating (accurate/inaccurate) to the location and size\nof the bounding box for each of the annotated objects. The average\npercentage of objects rated as accurate was 71.54% ( SD=29.73%).\nThis wide standard deviation reflects considerable variation in how\nparticipants perceived the annotation results.\nSome participants found the model\u2019s output to be satisfactory\nand accurate for their needs. However, others expected more pre-\ncise geometric alignment, particularly with architectural elements.\nAs one building manager explained: \u201cso you end up with a larger\nbounding box and it\u2019s at an angle relative to where you\u2019re viewing\nthe individual item, as opposed to the item itself being a rectangular\nprism up in the ceiling\u201d (M2). Similarly, M5 expressed that he was\nhoping the bounding box would exactly fit the volume of the room:\n\u201cthe box doesn\u2019t line up with this atrium exactly\u201d . This contrast in\nexpectations was also contributed by the diverse scale of objects\nparticipants labeled, ranging from entire spaces like atria to small\nitems such as electrical panels. These findings suggest that while\nthe current model quality is sufficient for many use cases, expecta-\ntions for geometric precision vary considerably across users and\nobject types.\n5.5.3 Concerns about Drone Flying Technology. When asked if they\nhad concerns about flying the drone, participants were relatively\nrelaxed. They primarily focused on appropriate scanning locations\nand privacy considerations rather than expressing significant safety\nworries. Participants suggested conducting scans during off-hours\nand limiting them to public areas: \u201ccommon spaces would be [ap-\npropriate to scan]. Like we could do the hallways, we could do the\nlarge atria\u201d (M1). Multiple building managers (M1, M2, M3) em-\nphasized avoiding sensitive spaces such as deans\u2019 offices, research\nlabs, and dormitories. Privacy emerged as a key consideration. M4\nrecommended providing advance notice to building tenants before\nscanning so individuals could avoid the area if they did not wish\nto be included. Building occupants raised concerns about facial\nprivacy, with one suggesting that \u201cmaybe it\u2019s best to blur people\u2019s\nfaces before annotation\u201d (O5).5.5.4 Diverse Use Cases and Stakeholder Benefits. Participants\ndemonstrated diverse needs that could be fulfilled by Fly-\nMeThrough. Building managers and occupants identified distinct\nuse cases, with several participants recognizing potential benefits\nbeyond their own user group.\nFor Building Managers. Building managers identified numer-\nous practical applications for FlyMeThrough, ranging from daily\noperational tasks to strategic space planning and resource man-\nagement. Our interviews revealed that FlyMeThrough has strong\npotential to enhance current mapping practices. One manager high-\nlighted how the system could eliminate redundant site visits: \u201cSome-\ntimes I\u2019ll go look at a room or space and then if I don\u2019t take a picture\nI go back to my office. If I forget then I just have to go back. But\nit\u2019s nice to have all the data right here\u201d (M1). This suggests that\nhaving comprehensive spatial data readily available could improve\nworkflow efficiency. The system\u2019s potential for remote inspection\nwas particularly valued for maintenance operations. M1 explained:\n\u201cIt just saves us from having to go check on things, for example, there\u2019s\na leak from the roof\u201d . Another specifically highlighted specialized\napplications: \u201cThis would come in most handy from a facilities per-\nspective if we did the 3D mapping in a mechanical room, where all\nthe pumps and valves, piping and stuff like that\u201d (M5). These com-\nments suggest that the system could reduce the need for in-person\ninspections of routine issues and provide valuable documentation\nof complex mechanical spaces.\nBeyond maintenance, managers saw potential for enhancing\nevent planning and coordination. One manager shared: \u201cAll the\nimages I have of the atrium are just me taking pictures with my\nphone from different angles. But it could be really helpful to just have\nthis. Then for example, if I have an event coming in, I could say to\nthem, here\u2019s the space. We could mount lights here on this column\nand this column\u201d (M1). Training applications were also identified as\nvaluable use cases. As M3 noted: \u201cIf for whatever reason I cannot meet\nwith somebody for a health and safety training, I could do it virtually\nwith them or for them to be able to review that space after the safety\ntraining as a records\u201d (M3). This points to potential educational\napplications beyond basic navigation and space management.\nAt a more strategic level, managers recognized the system\u2019s\npotential for space utilization analysis. As M2 noted: \u201cWhat\u2019s the\nsquare footage that we\u2019re using for storage versus classrooms versus\noffices versus labs? Those are things that are actually important to\nthe university from a sense of when grants go in and charging rates\nto grants for infrastructure\u201d (M2). This highlights FlyMeThrough\u2019s\npotential to support not only operational decision-making but also\nfinancial planning and resource allocation.\nFor Building Occupants Building occupants focused primarily\non navigation applications. One noted potential use in large, com-\nplex spaces: \u201cFor conference, if you go to convention center, it\u2019s super\nhuge. Also for direction\u201d (O2). The same participant highlighted\naccessibility benefits: \u201cMaybe if I went to a new country that I don\u2019t\nspeak their language, but I want to go to a museum. I want to see a spe-\ncific artifacts, I want to go to a specific section. I can just use this. Not\nreading anything\u201d (O2). This suggests that visual-based navigation\ncould overcome language barriers in unfamiliar environments.\nEssential amenities and safety information were prioritized by\noccupants: \u201cWhen I go to a new building, I always want to know the\nemergency exit and also bathrooms and water. Those are the most\n\n--- Page 11 ---\nFlyMeThrough\nimportant thing. And whether there are elevators or stairs to go to\nthe other place\u201d (O5). This emphasizes the importance of including\nbasic amenities and safety features in the annotation system.\nCross-User Benefits Notably, many participants recognized\npotential benefits beyond their immediate needs. One manager\narticulated how the system could serve multiple stakeholders: \u201cIt\nwould be the kind of space that would actually work really well\nfor people at all level of the department. Grad students could see\ntheir office before they move in. Facilities could get a chance to look\nat a space without having to spend 30 minutes getting a car and\ndriving across campus. It could work in group meetings talking about\nspace allocation, you could pull this up and have a conversation in a\nmeeting space as opposed to having to walk through it. So everybody\nthat uses the space could find a way to benefit from it\u201d (M2). This\nperspective suggests that FlyMeThrough has the potential to serve\nas a unified spatial information platform that bridges the needs of\ndifferent stakeholder groups, from administrators and facilities staff\nto everyday users and visitors.\n6 DISCUSSION & FUTURE WORK\nIn this paper, we introduce FlyMeThrough , a drone-based indoor\nmapping system that leverages commodity drones to scan indoor\nspaces and creates interactive and POI-infused 3D indoor maps.\nOur evaluation among 12 indoor spaces and 10 users showcased\nthe overall performance and usability of FlyMeThrough, and also\nrevealed the potential future impact and application scenarios.\n6.1 Application Scenarios\nFlyMeThrough demonstrates a wide range of promising application\nscenarios that can assist building managers, occupants, and even\nfirst-time visitors in knowing, evaluating, and inspecting indoor\nspaces. The tool supports tasks such as navigation, locating rooms,\nand identifying smaller indoor features\u2014from doors and stairs to\nelectrical panels and AEDs. This versatility highlights its strong\npotential for broader adoption, especially given FlyMeThrough\u2019s\nsupport for customizable mapping. With a quick annotation ses-\nsion lasting just a few minutes, users can generate tailored indoor\nmaps to fit specific needs, including multiple versions for different\nuse cases. The system also supports annotation of smaller, flexi-\nble, and temporary structures\u2014such as furniture, cargo boxes, and\nexhibition setups\u2014which are often omitted from traditional archi-\ntectural maps. This enables the creation of detailed management\nmaps for events, exhibitions, and furniture logistics. In addition,\nthe high-resolution 3D maps provide rich spatial cues that can ben-\nefit individuals with accessibility needs, allowing them to better\nunderstand environmental conditions and plan their movements\nbased on specific accessibility considerations.\n6.2 End-user Interface\nAt this stage, we have implemented a web interface (Figure 6) to\nshowcase the final 3D map. In future work, we plan to develop\nadditional UIs that reflect user study feedback and are tailored\nto different user needs. For example, a navigation interface could\nbetter support spatial movement and route planning to specific\nindoor facilities; a facility management interface could focus on ac-\ncessing and managing building safety information and equipment;and a mobile interface could support real-time user localization\nby matching camera feeds with the 3D indoor map to enable live\nnavigation assistance. We also envision accessibility-focused in-\nterfaces that evaluate and visualize spatial accessibility based on\nindividual user requirements. Furthermore, we expect to further\nautomate the annotation process by incorporating confirmations\nof open-vocabulary object detection results. Informed by our user\nstudy findings and the rich capabilities of our 3D mapping system,\nthere is significant potential to explore a wide design space for\nfuture 3D mapping interfaces.\n6.3 Improve model quality & reconstruction\nefficiency\nAs discussed in the user study and limitations, the current 3D\nmodels generated using the commercial photogrammetry tool\nMetaShape [ 39] can be improved in terms of quality. In future\nwork, we aim to explore alternative 3D reconstruction methods\nthat are fully open-source, better automated, and capable of pro-\nducing higher-quality 3D maps.\nOpen-source methods are improving rapidly\u2014for example, re-\ncent work such as VGGT-SLAM [31] already supports processing\nhundreds of frames efficiently. While our evaluations (Section 3.2)\nfound that existing open-source solutions did not yet meet our\nquality and robustness needs, our pipeline\u2019s modular design allows\nus to seamlessly integrate improved reconstruction algorithms as\nthey become viable. Another promising direction is generative view\nsynthesis [ 66], which enables the synthesis of novel views from\ncaptured images. This capability can help enhance reconstruction\nquality in areas that are occluded or otherwise inaccessible to the\ndrone, and potentially reduce the extent of capture required.\nWe plan to continue exploring these open-source alternatives and\nwill consider transitioning to them as their performance becomes\nsuitable for our use cases.\n6.4 Privacy Guidelines\nOur user study reveals both concerns and mitigation methods\naround privacy and confidentiality of the scanned indoor spaces.\nBuilding upon our findings and recognizing the importance of pri-\nvacy in indoor space scanning processes [ 15,35,69], we propose\nthe following privacy guidelines:\nSpatial Exclusion . Involve the key facility stakeholders ( e.g.\nbuilding management, building tenants) to maintain an exclusion\nlist where the drone fly should not cover. For example, private units,\nconfidential offices and labs, restrooms, etc.\nTemporal Minimization . Schedule flights outside peak occu-\npancy windows and publish a time-bounded flight schedule at least\n48 hours in advance so tenants can vacate or opt out. Inadvertent\ncapture and acoustic disturbance can also be minimized.\nData Management . The recorded data should be automatically\nfiltered for personal identification information ( e.g.faces) and cer-\ntain building information ( e.g.confidential text that need to be\nomitted) prior to further processing. Store unredacted footage on\nan encrypted, access-controlled server. Limit the access to the raw\ndata and processed 3D mapping data based on the building type\nand mapping goals.\n\n--- Page 12 ---\nXia Su, Ruiqi Chen, Jingwei Ma, Chu Li, and Jon E. Froehlich\n6.5 Automating Drone Flights\nIn future work, we will explore the automation of indoor drone\nflights to create a fully automated data collection pipeline, enabling\nregular, automated scans. Existing research [ 17,18,27,30,73] has\nalready explored autonomous path-planning for drones navigating\nindoor environments, and we plan to build upon them to explore\nthe feasibility of automating commodity drones at the software\nlevel. Based on our user study findings, we anticipate no major\nbystander concerns with automated drone data collection. The\nprimary challenges lie in ensuring appropriate scanning coverage\nand timing, as well as effective obstacle avoidance to guarantee\nflight safety. To address these concerns, we propose methods such\nas setting no-fly zones within indoor spaces and detecting when\nthe environment is too congested for safe flight, thereby mitigating\npotential risks.\n6.6 Comparing with commercial and\nLiDAR-based systems\nWhile direct empirical comparisons with commercial systems such\nas Matterport or LiDAR-based drones were not feasible due to\nproprietary restrictions and hardware availability, we examined\npublicly available documentation and prior studies to approximate\nhow our method compares to existing alternatives.\nEfficiency emerged as a key advantage of our approach. Matter-\nport and terrestrial laser scanning (TLS) systems typically require\n30\u201390 minutes to scan a 100 m 2indoor space [ 9], whereas our\nsystem captured spaces more than 20 times larger in less than\n10 minutes ( Table 1), demonstrating an improvement in acquisi-\ntion speed. This efficiency, combined with the ability to handle\nlarge-scale and complex indoor environments without additional\nhardware, highlights the practical benefits of our pipeline in time-\nsensitive or resource-constrained scenarios. Furthermore, among\nall methods tested in our experiments, our pipeline was the only\none capable of producing reliable reconstructions across such large\nspaces, indicating not only efficiency but also robustness.\nWe acknowledge that this efficiency comes at a cost to reconstruc-\ntion quality. When comparing the reconstruction results with the\nraw input images, our average peak signal-to-noise ratio (PSNR)[ 19]\nwas approximately 10 dB, which is lower than the 15\u201323 dB reported\nby Burde et al. [7] for 11 other methods on smaller-scale indoor\nscenes. This reflects a trade-off between speed and geometric fi-\ndelity that we aim to address in future work.\nIt is important to note that these comparisons are approximate, as\nthe methods differ in goals, assumptions, and testing environments.\nNonetheless, they offer a high-level perspective on the strengths and\nlimitations of our system in comparison to the broader landscape\nof indoor mapping technologies.\n7 CONCLUSION\nIn this paper, we present FlyMeThrough , a drone-based indoor map-\nping system that leverages RGB-only drone footage and human-AI\ncollaborative annotation to generate POI-infused 3D indoor maps.\nWe evaluated the system in 12 indoor spaces of varying sizes, func-\ntions, and spatial layouts. A user study involving building managers\nand occupants demonstrated both high technical performance and\nstrong usability of our system. Additionally, participants proposed awide range of potential use cases, particularly in support of building\nmanagement tasks.\nACKNOWLEDGMENTS\nThis research was supported by the University of Washington\u2019s\nCenter for Research and Education on Accessible Technology and\nExperiences (CREATE) and NSF grant #2125087. We give special\nthanks to our study participants for their participation and feedback.\nREFERENCES\n[1] [n. d.]. Capture, share, and collaborate the built world in immersive 3D. https:\n//matterport.com/\n[2]Meta AI. 2024. Segment Anything Model 2 (SAM2). https://github.com/\nfacebookresearch/sam2. Accessed: 2025-04-06.\n[3] Stanislav Alexovi\u010d, Milan Lacko, and J\u00e1n Ba\u010d\u00edk. 2023. 3D Mapping with a Drone\nEquipped with a Depth Camera in Indoor Environment. Acta Electrotechnica et\nInformatica 23, 1 (2023), 18\u201324.\n[4] Mohammed Eunus Ali, Muhammad Aamir Cheema, Tanzima Hashem, Anwaar\nUlhaq, and Muhammad Ali Babar. 2024. Enabling spatial digital twins: Technolo-\ngies, challenges, and future research directions. PFG\u2013Journal of Photogrammetry,\nRemote Sensing and Geoinformation Science (2024), 1\u201318.\n[5]Apple. 2022. RoomPlan - Augmented Reality. https://developer.apple.com/\naugmented-reality/roomplan/\n[6] Aleksei Bochkovskii, Ama\u00c3 G,l Delaunoy, Hugo Germain, Marcel Santos, Yichao\nZhou, Stephan R Richter, and Vladlen Koltun. 2024. Depth pro: Sharp monocular\nmetric depth in less than a second. arXiv preprint arXiv:2410.02073 (2024).\n[7] Varun Burde, Assia Benbihi, Pavel Burget, and Torsten Sattler. 2025. Comparative\nEvaluation of 3D Reconstruction Methods for Object Pose Estimation. In 2025\nIEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . IEEE,\n7669\u20137681.\n[8] Kathy Charmaz. 2006. Constructing grounded theory: A practical guide through\nqualitative analysis . sage.\n[9]Yuwei Chen, Jian Tang, Changhui Jiang, Lingli Zhu, Matti Lehtom\u00e4ki, Harri\nKaartinen, Risto Kaijaluoto, Yiwu Wang, Juha Hyypp\u00e4, Hannu Hyypp\u00e4, et al .\n2018. The accuracy comparison of three simultaneous localization and mapping\n(SLAM)-based indoor mapping technologies. Sensors 18, 10 (2018), 3228.\n[10] Yixin Chen, Guoxi Zhang, Yaowei Zhang, Hongming Xu, Peiyuan Zhi, Qing Li,\nand Siyuan Huang. 2024. SYNERGAI: Perception Alignment for Human-Robot\nCollaboration. arXiv preprint arXiv:2409.15684 (2024).\n[11] Cupix Inc. 2025. Cupix: 3D Digital Twin Solutions. https://www.cupix.com.\nAccessed: 2025-04-06.\n[12] Darko Dimitrov, Christian Knauer, Klaus Kriegel, and G\u00fcnter Rote. 2006. On the\nbounding boxes obtained by principal component analysis. In 22nd European\nWorkshop on Computational Geometry . 193\u2013196.\n[13] Esri. 2025. ArcGIS Indoors: Indoor Mapping and Space Management. https:\n//www.esri.com/en-us/arcgis/products/arcgis-indoors/overview Accessed: 2025-\n02-14.\n[14] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, Xiaowei Xu, et al .1996. A density-\nbased algorithm for discovering clusters in large spatial databases with noise. In\nkdd, Vol. 96. 226\u2013231.\n[15] Amir Fathalizadeh, Vahideh Moghtadaiee, and Mina Alishahi. 2024. Indoor\nlocation fingerprinting privacy: A comprehensive survey. arXiv preprint\narXiv:2404.07345 (2024).\n[16] Flyability. 2025. Elios 3: The Ultimate Indoor Drone for Inspection and Mapping.\nhttps://www.flyability.com/elios-3 Accessed: 2025-02-14.\n[17] Chuanxiang Gao, Xinyi Wang, Xi Chen, and Ben M Chen. 2024. A hierarchical\nmulti-UAV cooperative framework for infrastructure inspection and reconstruc-\ntion. Control Theory and Technology 22, 3 (2024), 394\u2013405.\n[18] Chuanxiang Gao, Xinyi Wang, Ruoyu Wang, Zuoquan Zhao, Yu Zhai, Xi Chen,\nand Ben M Chen. 2023. A UAV-based explore-then-exploit system for autonomous\nindoor facility inspection and scene reconstruction. Automation in Construction\n148 (2023), 104753.\n[19] Alain Hore and Djemel Ziou. 2010. Image quality metrics: PSNR vs. SSIM. In\n2010 20th international conference on pattern recognition . IEEE, 2366\u20132369.\n[20] Liubo Hou, Zhongliang Deng, Boyang Lou, Xiangyu Zhen, and Licheng Wei.\n2024. VGaussian: Dense Mapping of Indoor Drone Based on 3D Gaussian. 2024\nIEEE International Conference on Unmanned Systems (ICUS) (2024), 896\u2013901.\nhttps://api.semanticscholar.org/CorpusID:275786760\n[21] Dongki Jung, Jaehoon Choi, Yonghan Lee, and Dinesh Manocha. 2025. IM360:\nTextured Mesh Reconstruction for Large-scale Indoor Mapping with 360\u25e6Cam-\neras. arXiv:2502.12545 [cs.CV] https://arxiv.org/abs/2502.12545\n[22] Samer Karam, Francesco Nex, Bhanu Teja Chidura, and Norman Kerle. 2022.\nMicrodrone-based indoor mapping with graph slam. Drones 6, 11 (2022), 352.\n\n--- Page 13 ---\nFlyMeThrough\n[23] S Karam, F Nex, O Karlsson, J Rydell, E Bilock, M Tulldahl, M Holmberg, and N\nKerle. 2022. Micro and macro quadcopter drones for indoor mapping to support\ndisaster management. ISPRS Annals of the Photogrammetry, Remote Sensing and\nSpatial Information Sciences 1 (2022), 203\u2013210.\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis.\n2023. 3d gaussian splatting for real-time radiance field rendering. ACM Trans.\nGraph. 42, 4 (2023), 139\u20131.\n[25] Maksim Kolodiazhnyi, Anna Vorontsova, Matvey Skripkin, Danila Rukhovich,\nand Anton Konushin. 2024. UniDet3D: Multi-dataset Indoor 3D Object Detection.\narXiv preprint arXiv:2409.04234 (2024).\n[26] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin\nDehghan. 2024. Cubify Anything: Scaling Indoor 3D Object Detection. arXiv\npreprint arXiv:2412.04458 (2024).\n[27] Fangyu Li, Sisi Zlatanova, Martijn Koopman, Xueying Bai, and Abdoulaye Diakit\u00e9.\n2018. Universal path planning for an indoor drone. Automation in Construction\n95 (2018), 275\u2013283.\n[28] Xinlei Liu, Kevin Wu, Minchu Kulkarni, Michael Saugstad, Peyton Anton Rapo,\nJeremy Freiburger, Maryam Hosseini, Chu Li, and Jon E Froehlich. 2024. Towards\nFine-Grained Sidewalk Accessibility Assessment with Deep Learning: Initial\nBenchmarks and an Open Dataset. In Proceedings of the 26th International ACM\nSIGACCESS Conference on Computers and Accessibility . 1\u201312.\n[29] Luma AI. 2025. Luma AI: AI Video and 3D Capture Solutions. https://lumalabs.ai/.\nAccessed: 2025-04-06.\n[30] Mehdi Maboudi, MohammadReza Homaei, Soohwan Song, Shirin Malihi, Mo-\nhammad Saadatseresht, and Markus Gerke. 2023. A review on viewpoints and\npath planning for UAV-based 3-D reconstruction. IEEE Journal of Selected Topics\nin Applied Earth Observations and Remote Sensing 16 (2023), 5026\u20135048.\n[31] Dominic Maggio, Hyungtae Lim, and Luca Carlone. 2025. Vggt-slam: Dense rgb\nslam optimized on the sl (4) manifold. arXiv preprint arXiv:2505.12549 (2025).\n[32] Aman Malhotra and Ulrik S\u00f6derstr\u00f6m. 2022. Fixed camera drone based pho-\ntogrammetry for indoor mapping. In 2022 IEEE 9th Uttar Pradesh Section Interna-\ntional Conference on Electrical, Electronics and Computer Engineering (UPCON) .\nIEEE, 1\u20135.\n[33] Mappedin Inc. 2025. Mappedin: Indoor Mapping and Navigation Platform. https:\n//www.mappedin.com. Accessed: 2025-04-06.\n[34] Mapsted Inc. 2025. Mapsted: Indoor Positioning and Navigation Solutions. https:\n//mapsted.com. Accessed: 2025-04-06.\n[35] Rachel McAmis and Tadayoshi Kohno. 2023. The Writing on the Wall and 3D Dig-\nital Twins: Personal Information in (not so) Private Real Estate. In 32nd USENIX\nSecurity Symposium (USENIX Security 23) . USENIX Association, Anaheim, CA,\n2169\u20132186. https://www.usenix.org/conference/usenixsecurity23/presentation/\nmcamis\n[36] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi\nRamamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance\nfields for view synthesis. Commun. ACM 65, 1 (2021), 99\u2013106.\n[37] Riku Murai, Eric Dexheimer, and Andrew J. Davison. 2024. MASt3R-SLAM:\nReal-Time Dense SLAM with 3D Reconstruction Priors. arXiv preprint (2024).\n[38] NavVis GmbH. 2025. NavVis: Indoor Mapping and Navigation Solutions. https:\n//www.navvis.com. Accessed: 2025-04-06.\n[39] Jin-Si R Over, Andrew C Ritchie, Christine J Kranenburg, Jenna A Brown,\nDaniel D Buscombe, Tom Noble, Christopher R Sherwood, Jonathan A Warrick,\nand Phillipe A Wernette. 2021. Processing coastal imagery with Agisoft Metashape\nProfessional Edition, version 1.6\u2014Structure from motion workflow documentation .\nTechnical Report. US Geological Survey.\n[40] Onur \u00d6zye\u015fil, Vladislav Voroninski, Ronen Basri, and Amit Singer. 2017. A\nsurvey of structure from motion*. Acta Numerica 26 (2017), 305\u2013364.\n[41] Georgios Pipelidis, Christian Prehofer, and Ilias Gerostathopoulos. 2019. Boot-\nstrapping the dynamic generation of indoor maps with crowdsourced smart-\nphone sensor data. In Geographical Information Systems Theory, Applications and\nManagement: Third International Conference, GISTAM 2017, Porto, Portugal, April\n27\u201328, 2017, Revised Selected Papers 3 . Springer, 70\u201384.\n[42] Pointr. 2025. Pointr: Indoor Location Technology. https://www.pointr.tech/\nAccessed: 2025-02-14.\n[43] Polycam. [n. d.]. Polycam. https://poly.cam/. Accessed: 2024-07-02.\n[44] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali,\nTengyu Ma, Haitham Khedr, Roman R\u00e4dle, Chloe Rolland, Laura Gustafson,\nEric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan\nWu, Ross Girshick, Piotr Doll\u00e1r, and Christoph Feichtenhofer. 2024. SAM 2:\nSegment Anything in Images and Videos. arXiv preprint arXiv:2408.00714 (2024).\nhttps://arxiv.org/abs/2408.00714\n[45] David Rhind. 1988. A GIS research agenda. International Journal of Geographical\ninformation system 2, 1 (1988), 23\u201328.\n[46] D\u00e1vid Rozenberszki, G\u00e1bor S\u00f6r\u00f6s, Szilvia Szeier, and Andr\u00e1s L\u0151rincz. 2021. 3d se-\nmantic label transfer in human-robot collaboration. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision . 2602\u20132611.\n[47] Manaswi Saha, Michael Saugstad, Hanuma Teja Maddali, Aileen Zeng, Ryan\nHolland, Steven Bower, Aditya Dash, Sage Chen, Anthony Li, Kotaro Hara, et al .\n2019. Project sidewalk: A web-based crowdsourcing tool for collecting sidewalkaccessibility data at scale. In Proceedings of the 2019 CHI Conference on Human\nFactors in Computing Systems . 1\u201314.\n[48] Tejaswi Samavedula, Satyajit Mohapatra, and Sanjeet Kumar Nayak. 2025. Mini\nMapper: Cost-Effective Indoor Mapping and Navigation using Nano Drone.\nIn2025 17th International Conference on COMmunication Systems and NET-\nworks (COMSNETS) . 1377\u20131379. https://doi.org/10.1109/COMSNETS63942.2025.\n10885580\n[49] Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. 2016. Structure-from-Motion\nRevisited. In Conference on Computer Vision and Pattern Recognition (CVPR) .\n[50] Johannes L Schonberger and Jan-Michael Frahm. 2016. Structure-from-motion\nrevisited. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition . 4104\u20134113.\n[51] Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys.\n2016. Pixelwise view selection for unstructured multi-view stereo. In Com-\nputer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 11-14, 2016, Proceedings, Part III 14 . Springer, 501\u2013518.\n[52] Johannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael\nFrahm. 2016. Pixelwise View Selection for Unstructured Multi-View Stereo. In\nEuropean Conference on Computer Vision (ECCV) .\n[53] Guanlin Shen, Jingwei Huang, Zhihua Hu, and Bin Wang. 2024. Cn-rma: Com-\nbined network with ray marching aggregation for 3d indoor object detection\nfrom multi-view images. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition . 21326\u201321335.\n[54] Manli Shu, Le Xue, Ning Yu, Roberto Mart\u00edn-Mart\u00edn, Caiming Xiong, Tom Gold-\nstein, Juan Carlos Niebles, and Ran Xu. 2024. Hierarchical Point Attention for\nIndoor 3D Object Detection. In 2024 IEEE International Conference on Robotics\nand Automation (ICRA) . IEEE, 4245\u20134251.\n[55] Skydio. 2025. Introducing 3D Indoor Capture. https://www.skydio.com/blog/\nintroducing-3d-indoor-capture Accessed: 2025-02-14.\n[56] Xu Song, Xuan Liang, and Zhou Huaidong. 2025. Semantic mapping techniques\nfor indoor mobile robots: Review and prospect. Measurement and Control 58, 3\n(2025), 377\u2013393.\n[57] Peter Sturm. 2021. Pinhole camera model. In Computer Vision: A Reference Guide .\nSpringer, 983\u2013986.\n[58] Xia Su, Daniel Campos Zamora, and Jon E Froehlich. 2024. RAIS: Towards A\nRobotic Mapping and Assessment Tool for Indoor Accessibility Using Commodity\nHardware. In Proceedings of the 26th International ACM SIGACCESS Conference\non Computers and Accessibility . 1\u20135.\n[59] Xia Su, Ruiqi Chen, Weiye Zhang, Jingwei Ma, and Jon E Froehlich. 2024. A Demo\nof DIAM: Drone-based Indoor Accessibility Mapping. In Adjunct Proceedings of\nthe 37th Annual ACM Symposium on User Interface Software and Technology . 1\u20133.\n[60] Xia Su, Han Zhang, Kaiming Cheng, Jaewook Lee, Qiaochu Liu, Wyatt Olson,\nand Jon E Froehlich. 2024. RASSAR: Room Accessibility and Safety Scanning in\nAugmented Reality. In Proceedings of the CHI Conference on Human Factors in\nComputing Systems . 1\u201317.\n[61] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang,\nAlexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al .2023.\nNerfstudio: A modular framework for neural radiance field development. In ACM\nSIGGRAPH 2023 conference proceedings . 1\u201312.\n[62] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexan-\nder Schwing, and Zhicheng Yan. 2024. MV-DUSt3R+: Single-Stage Scene Re-\nconstruction from Sparse Views In 2 Seconds. arXiv preprint arXiv:2412.06974\n(2024).\n[63] Zachary Teed and Jia Deng. 2021. Droid-slam: Deep visual slam for monocular,\nstereo, and rgb-d cameras. Advances in neural information processing systems 34\n(2021), 16558\u201316569.\n[64] Ultralytics. 2023. YOLOv8: Cutting-edge object detection. https://github.com/\nultralytics/ultralytics. Accessed: 2025-04-06.\n[65] Jiangyi Wang and Na Zhao. 2025. Uncertainty Meets Diversity: A Comprehensive\nActive Learning Framework for Indoor 3D Object Detection. arXiv preprint\narXiv:2503.16125 (2025).\n[66] Ethan Weber, Norman M\u00fcller, Yash Kant, Vasu Agrawal, Michael Zollh\u00f6fer,\nAngjoo Kanazawa, and Christian Richardt. 2025. Fillerbuster: Multi-View Scene\nCompletion for Casual Captures. arXiv:2502.05175.\n[67] Galen Weld, Esther Jang, Anthony Li, Aileen Zeng, Kurtis Heimerl, and Jon E\nFroehlich. 2019. Deep learning for automatically detecting sidewalk accessibility\nproblems using streetscape imagery. In Proceedings of the 21st International ACM\nSIGACCESS Conference on Computers and Accessibility . 196\u2013209.\n[68] Yating Xu, Chen Li, and Gim Hee Lee. 2024. MVSDet: Multi-View Indoor 3D Ob-\nject Detection via Efficient Plane Sweeps. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems .\n[69] Mingxu Yang, Chuhua Huang, Xin Huang, and Shengjin Hou. 2025. Privacy-\nPreserved Visual Simultaneous Localization and Mapping Based on a Dual-\nComponent Approach. Applied Sciences 15, 5 (2025), 2583.\n[70] Han Yue and Hangbin Wu. 2018. Update Method of Indoor Maps Based on\nVolunteered Geographic Information (VGI). In 2018 26th International Conference\non Geoinformatics . IEEE, 1\u20135.\n\n--- Page 14 ---\nXia Su, Ruiqi Chen, Jingwei Ma, Chu Li, and Jon E. Froehlich\n[71] Jeonghyeon Yun and Byungwoon Park. 2024. A GNSS/Barometric Altimeter\nTightly Coupled Integration for Three-Dimensional Semi-Indoor Mapping With\nAndroid Smartphones. IEEE Geoscience and Remote Sensing Letters 21 (2024), 1\u20135.\nhttps://doi.org/10.1109/LGRS.2024.3365610\n[72] Jing Zeng, Qi Ye, Tianle Liu, Yang Xu, Jin Li, Jinming Xu, Liang Li, and Jiming\nChen. 2024. Multi-robot autonomous 3D reconstruction using Gaussian splatting\nwith Semantic guidance. arXiv preprint arXiv:2412.02249 (2024).\n[73] Hao Xuan Zhang, Yilin Yang, and Zhengbo Zou. 2024. ICON drone: Autonomous\nindoor exploration using Unmanned Aerial Vehicle for semantic 3D reconstruc-\ntion. In Proceedings of the 11th ACM International Conference on Systems for\nEnergy-Efficient Buildings, Cities, and Transportation . 66\u201376.[74] Xiaohong Zhang, Huisheng Ye, Jingwen Li, Qinyu Tang, Yuanqi Li, Yanwen Guo,\nand Jie Guo. 2024. Prompt3D: Random Prompt Assisted Weakly-Supervised 3D\nObject Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 28046\u201328055.\n[75] Guyue Zhou, Ang Liu, Kang Yang, Tao Wang, and Zexiang Li. 2014. An embedded\nsolution to visual mapping for consumer drones. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops . 656\u2013661.\n[76] Rui Zhu, Cogan Shimizu, Shirly Stephen, Colby K Fisher, Thomas Thelen, Kitty\nCurrier, Krzysztof Janowicz, Pascal Hitzler, Mark Schildhauer, Wenwen Li, et al .\n2025. The KnowWhereGraph: A Large-Scale Geo-Knowledge Graph for In-\nterdisciplinary Knowledge Discovery and Geo-Enrichment. arXiv preprint\narXiv:2502.13874 (2025).",
  "project_dir": "artifacts/projects/enhanced_cs.HC_2508.20034v1_FlyMeThrough_Human_AI_Collaborative_3D_Indoor_Map",
  "communication_dir": "artifacts/projects/enhanced_cs.HC_2508.20034v1_FlyMeThrough_Human_AI_Collaborative_3D_Indoor_Map/.agent_comm",
  "assigned_at": "2025-08-28T20:44:44.884489",
  "status": "assigned"
}